local OptimizerCaptioner, parent = torch.class("dp.OptimizerCaptioner", "dp.PropagatorCaptioner")
OptimizerCaptioner.isOptimizerCaptioner = true

function OptimizerCaptioner:__init(config)
    config = config or {}
    local args, loss, sampler, acc_update, callback, update_interval, stats,
    _cuda = xlua.unpack({ config },
        'OptimizerCaptioner',
        'Optimizes a model on a training dataset',
        {
            arg = 'loss',
            type = 'nn.Criterion',
            req = true,
            help = 'a neural network Criterion to evaluate or minimize'
        },
        {
            arg = 'sampler',
            type = 'dp.Sampler',
            help = 'used to iterate through the train set. ' ..
                    'Defaults to dp.ShuffleSampler()'
        },
        {
            arg = 'acc_update',
            type = 'boolean',
            default = false,
            help = 'when true, uses the faster accUpdateGradParameters, ' ..
                    'which performs an inplace update (no need for param gradients). ' ..
                    'However, this also means that Momentum, WeightDecay and other ' ..
                    'such gradient modifying Visitors cannot be used.'
        },
        {
            arg = 'callback',
            type = 'function',
            req = true,
            help = 'function(model, report) that does things like' ..
                    'update model, gather statistics, decay learning rate, etc.'
        },
        {
            arg = 'update_interval',
            type = 'number',
            default = 1,
            help = 'update the model every update_interval'
        },
        {
            arg = 'stats',
            type = 'boolean',
            default = true,
            help = 'display statistics'
        },
        {
            arg = '_cuda',
            type = 'boolean',
            help = 'use gpu or not'
        }
        )
    self._update_interval = update_interval
    self._acc_update = acc_update
    config.loss = loss
    config.callback = callback
    config.sampler = sampler or dp.ShuffleSampler()
    config.stats = stats
    parent.__init(self, config)
    --self._cuda = _cuda
end

function OptimizerCaptioner:setup(config)
    parent.setup(self, config)
    self._model:zeroGradParameters() -- don't forget this, else NaN errors
end

function OptimizerCaptioner:propagateBatch(batch, report)
    self._model:training()
    self:forward(batch)
    self:monitor(batch, report)
    self:backward(batch)
    if report.epoch % self._update_interval == 0 then
        self._callback(self._model, report)
    end
    self:doneBatch(report)
end

function OptimizerCaptioner:backward(batch)
    local input = batch:inputs():input()
    local target = batch:targets():input()
    target = self._target_module:forward(target)

    local timesteps = target[1]:size()[1]
    local sum_gradOutput = {}
    if self._cuda then
      sum_gradOutput[1] = torch.Tensor(timesteps,self.output[1]:size()[2]):zero():cuda()
      sum_gradOutput[2] = {}
      sum_gradOutput[2][1] = torch.Tensor(timesteps, self.output[1]:size()[2]):zero():cuda()
      sum_gradOutput[2][2] = torch.Tensor(1, timesteps):zero():cuda()
      for i = timesteps,1,-1 do --#TODO[done]: from t=16 to t=1
          local temp_i = {}
          temp_i[1] = torch.Tensor(1,self.output[1]:size()[2]):cuda()
          temp_i[2] = {}
          temp_i[2][1] = torch.Tensor(1,self.output[1]:size()[2]):cuda()
          temp_i[2][2] = torch.Tensor(1):cuda()
          
          temp_t = torch.Tensor(1) -- convert to tensor
          temp_t[1] = target[1][i]
          
          temp_i[1][1] = self.output[1][i]:cuda()
          temp_i[2][1][1] = self.output[2][1][i]:cuda()
          temp_i[2][2] = self.output[2][2][i]:cuda()
          
          temp_gradOutput = self._loss:backward(temp_i, temp_t)
          sum_gradOutput[1][i] = temp_gradOutput[1][1]
          sum_gradOutput[2][1][i] = temp_gradOutput[2][1][1]
          sum_gradOutput[2][2][1][i] = temp_gradOutput[2][2][1]
      end
    else
      sum_gradOutput[1] = torch.Tensor(timesteps,self.output[1]:size()[2]):zero()
      sum_gradOutput[2] = {}
      sum_gradOutput[2][1] = torch.Tensor(timesteps, self.output[1]:size()[2]):zero()
      sum_gradOutput[2][2] = torch.Tensor(1, timesteps):zero()
      for i = timesteps,1,-1 do --#TODO[done]: from t=16 to t=1
          local temp_i = {}
          temp_i[1] = torch.Tensor(1,self.output[1]:size()[2])
          temp_i[2] = {}
          temp_i[2][1] = torch.Tensor(1,self.output[1]:size()[2])
          temp_i[2][2] = torch.Tensor(1)
          
          temp_t = torch.Tensor(1) -- convert to tensor
          temp_t[1] = target[1][i]
          
          temp_i[1][1] = self.output[1][i]
          temp_i[2][1][1] = self.output[2][1][i]
          temp_i[2][2] = self.output[2][2][i]
          
          temp_gradOutput = self._loss:backward(temp_i, temp_t)
          sum_gradOutput[1][i] = temp_gradOutput[1][1]
          sum_gradOutput[2][1][i] = temp_gradOutput[2][1][1]
          sum_gradOutput[2][2][1][i] = temp_gradOutput[2][2][1]
      end
    end
    -- estimate gradient of loss w.r.t. outputs
    --self.gradOutput = self._loss:backward(self.output, target)

    self.gradOutput = sum_gradOutput
    
    -- backprop through model
    if self._include_target then
        input = { input, target }
    end
    if self._acc_update then
        self.gradInput = self._model:updateGradInput(input, self.gradOutput)
    else
        self.gradInput = self._model:backward(input, self.gradOutput)
    end
    -- so that visitors can known whether or not gradParams were updated
    self._model.dpnn_accGradParameters = not self._acc_update
end
